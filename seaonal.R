#####################Algorithm to prepare locality matrix####################################
library (tidyverse)
library(data.table)
library(adehabitatHR)
library(concaveman)
library(rgeos)
library(sf)
library(smoothr)


# Pre-processed location data with measurements betweeen eBird locations. This is generated by a julia script.
eloc <- readRDS("loc.RDS")
setDT(eloc)
setkey(eloc, LOCALITY1, LOCALITY2)


speciesattr <- read.csv2(paste0("SoIB_main_09062023.csv"), sep=",") %>% 
  dplyr::select('eBird.Scientific.Name.2022', 'India.Endemic', 'Endemic.Region', 'Migratory.Status.Within.India', 'Clip.Region')

colnames(speciesattr) <- c("SCIENTIFIC.NAME", "ENDEMIC", "ENDEMIC.REGION", "MIGRATION", "CLIP.REGION")

#speciesattr <-  speciesattr %>% filter (ENDEMIC == "Yes")
#speciesattr <-  speciesattr %>% filter (ENDEMIC.REGION == "Sri Lanka")
#speciesattr <-  speciesattr %>% filter (CLIP.REGION == "Pakistan")
speciesattr <-  speciesattr %>% filter (MIGRATION == "Winter Migrant",
                                        ENDEMIC == "No",
                                        CLIP.REGION == "None")

species <- speciesattr$SCIENTIFIC.NAME

#species <- readRDS(".\\data\\species.rds")

species <- read.csv("species.csv") 
species <- species[,1]

sp <- species[1]
#for (sp in species) 
#{
  # For each species, find the unique locations
  if (!file.exists(paste0(".\\data\\data_", sp,".rds"))) 
  { 
    print(paste("No file for",sp)) 
    next
  }
  
  data <- readRDS(paste0(".\\data\\data_", sp,".rds"))
  
  # Make the location data more coarse.
  spec_loc_table <- data %>% 
    mutate (LATITUDE = 0.1 * round(10 * LATITUDE),
            LONGITUDE = 0.1 * round (10 * LONGITUDE),
            LOCALITY_ID = as.integer (LONGITUDE*100000 + LATITUDE*10),
            MONTH = as.integer(format(as.Date(OBSERVATION.DATE),"%m")),
            YEAR = as.integer(format(as.Date(OBSERVATION.DATE),"%Y"))) %>%
    dplyr::select(LOCALITY_ID, LATITUDE, LONGITUDE, MONTH) %>% 
    distinct()
  spec_loc_table_sf <- st_as_sf(spec_loc_table, coords = c("LONGITUDE", "LATITUDE"), crs = 4326)
  
  data <- NULL
  
  split_months <- split(spec_loc_table, spec_loc_table$MONTH)
  
  for (month_data in split_months) {
    month_data$MONTH <- NULL
    
    # Assuming 'num_localities' is defined as the number of unique LOCALITY_ID in the dataset
    num_localities <- length(unique(month_data$LOCALITY_ID))
    
    print(paste("MONTH:",num_localities,"\n"))
    
    # Create a locality_matrix
    locality_list <- as.integer(month_data$LOCALITY_ID) %>% sort()

    loc_f <- eloc[LOCALITY1 %in% locality_list & LOCALITY2 %in% locality_list]
    
    loc_f$LOCALITY1 <- as.integer(loc_f$LOCALITY1)
    loc_f$LOCALITY2 <- as.integer(loc_f$LOCALITY2)
    
    # Combine locality values to create row and column names
    locations <- unique(c(loc_f$LOCALITY1, loc_f$LOCALITY2))
    # Generate complete set of row and column indices
    indices <- expand.grid(LOCALITY1 = locations, LOCALITY2 = locations, stringsAsFactors = FALSE) %>% as.data.table()
    
    locations <- NULL
    
    setDT(loc_f)
    setkey(loc_f, LOCALITY1, LOCALITY2)
    
    # Aggregate distances for the same row and column indices
    aggregated_loc_f <- loc_f[, .(DISTANCE = sum(DISTANCE)), by = .(LOCALITY1, LOCALITY2)]
    
    loc_f  <- NULL
    # Merge with the aggregated data to fill in missing values
    loc_f  <- merge(indices, aggregated_loc_f, by = c("LOCALITY1", "LOCALITY2"), all.x = TRUE)
    indices <- NULL
    aggregated_loc_f <- NULL
    
    setDT(loc_f)
    setkey(loc_f, LOCALITY1, LOCALITY2)
    
    # Create a locality_matrix
    locality_matrix <- matrix(
      loc_f$DISTANCE,
      nrow = num_localities,  
      ncol = num_localities,        
      byrow = TRUE
    )
    loc_f <- NULL
    rownames(locality_matrix) = colnames(locality_matrix) = locality_list
    locality_list <- NULL
    
    # Make the matrix symmetrical.
    missing_values <- is.na(locality_matrix)
    transposed_values <- t(locality_matrix)
    locality_matrix[missing_values] <- transposed_values[missing_values]
    
    # Step 3b: Find clusters for each of these locality_matrix
    n <- nrow(locality_matrix)
    
    cluster_numbers <- rep(0, n)  # Initialize cluster numbers
    next_cluster_number <- 1  # Start with cluster number 1
    dist <- 50
    
    for (loc in 1:n) {
      if (cluster_numbers[loc] == 0) {
        cluster_numbers[loc] <- next_cluster_number  # Assign cluster number
        
        # Iterate over locations within the same cluster
        locations_to_process <- loc
        while (length(locations_to_process) > 0) {
          current_location <- locations_to_process[1]
          locations_to_process <- locations_to_process[-1]
          
          # Find nearby locations within the distance threshold
          nearby_locations <- which(locality_matrix[current_location, ] <= dist)
          
          # Assign the same cluster number to nearby locations if not already assigned
          unassigned_nearby <- nearby_locations[cluster_numbers[nearby_locations] == 0]
          cluster_numbers[unassigned_nearby] <- next_cluster_number
          
          # Add unassigned nearby locations to the list for processing
          locations_to_process <- c(locations_to_process, unassigned_nearby)
        }
        
        next_cluster_number <- next_cluster_number + 1  # Increment cluster number
      }
    }
      
    # Create data frame with locality ID and cluster number
    cluster <- data.frame(
      LOCALITY.ID = rownames(locality_matrix),
      CLUSTER = cluster_numbers
    )
      
    colnames(cluster) <- c("LOCALITY.ID", "CLUSTER")
    cluster$LOCALITY.ID <- as.integer(cluster$LOCALITY.ID)     
      
    # We cant have polygons with two points. Plot them as just points.
    cluster <- cluster %>%
      group_by(CLUSTER) %>%
      mutate(CLUSTER = ifelse(n() < 3, NA, CLUSTER)) %>%
      ungroup()    
      
    # Calculate concave bounded polygon for each cluster
    clusters <- cluster$CLUSTER %>% unique()
    
    clusterPolygons <- list()
    clusterPolygonCount <- 0
    
    # Iterate over the clusters and create polygons, make it into a list.
    for (j in clusters) {  
      if (!is.na(j)) {
        loc3 <- cluster %>% 
          filter(CLUSTER == j) %>% 
          inner_join(month_data, by = c("LOCALITY.ID" = "LOCALITY_ID")) %>% 
#          mutate(LATITUDE = (LOCALITY.ID - 10000 * as.integer(LOCALITY.ID / 10000)) / 10,
#                 LONGITUDE = as.integer(LOCALITY.ID / 10000) / 10) %>%
          dplyr::select(LATITUDE, LONGITUDE) %>%
          distinct()
        
        sp::coordinates(loc3) <- ~LONGITUDE + LATITUDE
        CH <- gConvexHull(loc3)
        
        if ("polygons" %in% slotNames(CH)) {
          clusterPolygonCount <- clusterPolygonCount + 1
          
          dat_sf <- st_as_sf(loc3, coords = c("LONGITUDE", "LATITUDE"))
          concave <- concaveman(dat_sf, concavity = 1.0, length_threshold = 0.3)
          CH <- as_Spatial(concave)
          
#          r_poly_smooth <- smooth(CH, method = "chaikin")
          r_poly_smooth <- CH
          
          # Assigning unique ID
          r_poly_smooth@polygons[[1]]@ID <- as.character(clusterPolygonCount)
          
          clusterPolygons[[clusterPolygonCount]] <- r_poly_smooth
        } else {
          # Remove these points from the cluster as they lie in a straight line
          cluster <- cluster %>%
            mutate(CLUSTER = ifelse(CLUSTER == j, NA, CLUSTER))
        }
      }
    }
    print(clusterPolygonCount)   
#    # Remove the data points from spec_loc_table that lie inside any of the polygons
#    for (polygon in clusterPolygons) {
#      polygon_sf <- st_as_sf(polygon)
#      st_crs(polygon_sf) <- st_crs(spec_loc_table_sf)
#      inside_polygon <- st_contains(polygon_sf, spec_loc_table_sf, sparse = FALSE)
#      spec_loc_table_sf <- spec_loc_table_sf[!inside_polygon, ]
#      print(paste("Removed", grep(TRUE, inside_polygon)))
#      print(paste("REDUCED: ", nrow(spec_loc_table_sf), "\n"))
#      inside_polygon <- NULL
#      polygon_sf <- NULL
#    }
  }
#}